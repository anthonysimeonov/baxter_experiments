{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "#### 2. add rollouts periodically for network performance evaluation\n",
    "#### 5. high dimension observation space\n",
    "#### 6. fix BN bug (FIXED?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Policy Gradient (REINFORCE) and Actor Critic (DDPG)\n",
    "\n",
    "Name:\n",
    "\n",
    "ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise requires you to solve various continous control problems in OpenAI Gym. We will first apply Policy Gradient, and later extend this to an Actor Critic method. \n",
    "\n",
    "Specifically, you will impliment REINFORCE with batch update and Deep Deterministic Policy Gradient (DDPG) for low and high dimension observation space.\n",
    "\n",
    "You should test on 'InvertedPendulum-v1', 'Pendulum-v0', 'HalfCheetah-v1', 'Ant-v1', and 'Humanoid-v1'.\n",
    "\n",
    "REINFORCE is an on-policy method, this requires new samples to be collected for each update. Parralelizing this sample collection will increase speed. In contrast, DDPG is off-polciy and allows reuse of samples through a Replay Buffer like in DQN.\n",
    "\n",
    "REINFORCE: http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf\n",
    "\n",
    "DDPG: https://arxiv.org/pdf/1509.02971.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "#set GPU to use\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#useful libraries\n",
    "from tqdm import tqdm #gives progress bars\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "#environment functions\n",
    "import gym\n",
    "from gym import wrappers\n",
    "#import roboschool\n",
    "\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.utils as utils\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessory Functions\n",
    "\n",
    "These are useful functions in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "#exponential moving average\n",
    "#----------------------------------------------------\n",
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "#----------------------------------------------------\n",
    "#timing function\n",
    "#----------------------------------------------------\n",
    "class Timer(object):\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.tstart = time.time()\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        if self.name:\n",
    "            print('[%s]' % self.name,)\n",
    "        print('Elapsed: %s' % (time.time() - self.tstart))\n",
    "\n",
    "#----------------------------------------------------\n",
    "#flip a pytorch vector\n",
    "#----------------------------------------------------\n",
    "def flip(x, dim):\n",
    "    dim = x.dim() + dim if dim < 0 else dim\n",
    "    inds = tuple(slice(None, None) if i != dim\n",
    "             else x.new(torch.arange(x.size(i)-1, -1, -1).tolist()).long()\n",
    "             for i in range(x.dim()))\n",
    "    return x[inds]\n",
    "\n",
    "#----------------------------------------------------\n",
    "#normalize actions\n",
    "#----------------------------------------------------\n",
    "#https://github.com/openai/gym/blob/78c416ef7bc829ce55b404b6604641ba0cf47d10/gym/core.py\n",
    "\n",
    "class NormalizeAction(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        #tanh outputs (-1,1) from tanh, need to be [action_space.low,environment.high]\n",
    "        return (action + 1) / 2 * (self.action_space.high - self.action_space.low) + self.action_space.low\n",
    "        \n",
    "    def reverse_action(self, action):\n",
    "        #reverse of that above\n",
    "        return (action - self.action_space.low) / (self.action_space.high - self.action_space.low) * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment\n",
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE = False\n",
    "NUM_EPISODES=12000\n",
    "MAX_PATH_LENGTH = 500\n",
    "GAMMA=0.99\n",
    "BATCH_SIZE = 128\n",
    "SEED = 0\n",
    "min_timesteps_per_batch=2000\n",
    "\n",
    "logging_interval = 100\n",
    "animate_interval = logging_interval * 5\n",
    "num_runs = 5\n",
    "logdir='./tmp/'\n",
    "\n",
    "env_name = 'HalfCheetah-v2' \n",
    "#env_name = 'FetchReach-v1'\n",
    "#env_name='InvertedPendulum-v1'\n",
    "#env_name = 'Pendulum-v0'\n",
    "#env_name = \"RoboschoolHalfCheetah-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed\n",
    "### Wrap environment, log videos, setup cuda variables\n",
    "### Record action and observation space dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "No module named mujoco_py. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2b71531985bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Gym things\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mVISUALIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zephirefaith/baxter_ws/ros_learning/local/lib/python2.7/site-packages/gym/envs/registration.pyc\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zephirefaith/baxter_ws/ros_learning/local/lib/python2.7/site-packages/gym/envs/registration.pyc\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zephirefaith/baxter_ws/ros_learning/local/lib/python2.7/site-packages/gym/envs/registration.pyc\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zephirefaith/baxter_ws/ros_learning/local/lib/python2.7/site-packages/gym/envs/registration.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mentry_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEntryPoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zephirefaith/baxter_ws/ros_learning/local/lib/python2.7/site-packages/pkg_resources/__init__.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, require, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zephirefaith/baxter_ws/ros_learning/local/lib/python2.7/site-packages/pkg_resources/__init__.pyc\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2233\u001b[0m         \u001b[0mResolve\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentry\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mits\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m         \"\"\"\n\u001b[0;32m-> 2235\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__name__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2236\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zephirefaith/baxter_ws/ros_learning/local/lib/python2.7/site-packages/gym/envs/mujoco/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMujocoEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# ^^^^^ so that user gets the correct error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# message if mujoco is not installed correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAntEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf_cheetah\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHalfCheetahEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zephirefaith/baxter_ws/ros_learning/local/lib/python2.7/site-packages/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMujocoEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: No module named mujoco_py. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "# Gym things\n",
    "env = gym.make(env_name)\n",
    "if VISUALIZE:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "env._max_episodes_steps = MAX_PATH_LENGTH\n",
    "\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    print(\"This is a discrete action space, probably not the right algorithm to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "In this section you will implement REINFORCE, with modifications for batch training. It will be for use on both discrete and continous action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name='CartPole-v0'\n",
    "#env_name='MountainCar-v0'\n",
    "#env_name='Pendulum-v0'\n",
    "env_name='InvertedPendulum-v2'\n",
    "#env_name = 'HalfCheetah-v1'\n",
    "\n",
    "visualize = False\n",
    "animate=visualize\n",
    "\n",
    "n_iter=12000\n",
    "gamma=0.99\n",
    "learning_rate=1e-3\n",
    "\n",
    "min_timesteps_per_batch=2000\n",
    "max_path_length=None\n",
    "\n",
    "#saving parameters\n",
    "logdir='./tmp/'\n",
    "logging_interval = int(n_iter / 100)\n",
    "animate_interval = int(n_iter / 20)\n",
    "seed=0\n",
    "\n",
    "#tricks?\n",
    "reward_to_go=True\n",
    "normalize_advantages=True\n",
    "nn_baseline=False\n",
    "cumsum = True\n",
    "\n",
    "# network arguments\n",
    "n_layers=2\n",
    "size=32\n",
    "\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self,  \n",
    "            input_size, \n",
    "            output_size,\n",
    "            n_layers=1, \n",
    "            size=64, \n",
    "            activation='relu',\n",
    "            output_activation='softmax'):\n",
    "        \n",
    "        super(mlp, self).__init__()\n",
    "        \n",
    "        self.output_activation = output_activation\n",
    "        \n",
    "        #populate architecture list\n",
    "        self.architecture = []\n",
    "        #self.architecture.append(input_size)\n",
    "        for i in range(n_layers):\n",
    "            self.architecture.append(size)\n",
    "            #if i%2 == 0:\n",
    "            self.architecture.append(activation)\n",
    "                \n",
    "        self.architecture.append(output_size)\n",
    "        #self.architecture.append(output_activation)\n",
    "        \n",
    "        print('Network architecture is : {}'.format(self.architecture))\n",
    "        \n",
    "        #construct network\n",
    "        input_channels = input_size #initialize input_channels\n",
    "        self.layers = []\n",
    "        for layer in self.architecture:\n",
    "            output_channels = layer\n",
    "            if layer == 'tanh':\n",
    "                self.layers.append(nn.Tanh())  #NOT INPLACE? MIGHT CAUSE ISSUE OR BE OK?\n",
    "            \n",
    "            elif layer == 'relu':\n",
    "                self.layers.append(nn.ReLU(inplace=True))\n",
    "            elif layer == None:\n",
    "                pass\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(input_channels, output_channels))\n",
    "                input_channels = output_channels\n",
    "        \n",
    "        self.fc1 = nn.Sequential(*self.layers)\n",
    "        \n",
    "        if(self.output_activation == 'softmax'):\n",
    "            self.softmax = nn.Softmax(dim = 1)\n",
    "        self._initialize_weights()\n",
    "        #if initialization == 'Xavier':\n",
    "        \n",
    "        #    nn.init.xavier_uniform(self.conv1.weight)\n",
    "        #    nn.init.xavier_uniform(self.conv2.weight)\n",
    "\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        if(self.output_activation == 'softmax'):\n",
    "            x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "def pathlength(path):\n",
    "    return len(path[\"reward\"])\n",
    "\n",
    "def sample_action(logit, discrete):\n",
    "    if discrete:\n",
    "        m = Categorical(logit)\n",
    "        action = m.sample()\n",
    "        log_odds_action = m.log_prob(action)\n",
    "        return action, log_odds_action#, probability\n",
    "    else:\n",
    "        shape = int(logit.shape[1]/2)\n",
    "        mu = logit[:,:shape]\n",
    "        sigma = logit[:,shape:]\n",
    "        sigma = F.softplus(sigma)\n",
    "        \n",
    "        m = Normal(mu, sigma)\n",
    "        action = m.sample()\n",
    "        log_odds_action = m.log_prob(action)\n",
    "        #if math.isnan(log_odds_action):\n",
    "        #    print(log_odds_action)\n",
    "        #    print(action)\n",
    "        #    print(logit)\n",
    "            \n",
    "        return action, log_odds_action\n",
    "    \n",
    "def update_policy(paths, net, cumsum = 1):#ob_no, ac_na, rew, log_odd):\n",
    "    num_paths = len(paths)\n",
    "\n",
    "    rew_cums = []\n",
    "    log_odds = []\n",
    "    for path in paths:\n",
    "        rew = path['reward']\n",
    "        log_odd = path['log_odds']\n",
    "        \n",
    "        log_odds.append(log_odd)\n",
    "\n",
    "        rew = torch.Tensor(rew)\n",
    "        \n",
    "        rew_cum = flip(torch.cumsum(flip(rew,0),0),0) #make a matrix multiplication to incorporate the decreasing value too\n",
    "        if cumsum:\n",
    "            rew_cums.append(rew_cum)\n",
    "        else: #raw sum, not using reward to go\n",
    "            max_rew = torch.ones(len(rew)) * rew_cum[0]\n",
    "            #rew_cums.append(torch.sum(rew,0)*rew_cum)\n",
    "            #rew = (rew-rew+1)*torch.sum(rew,0)\n",
    "            rew_cums.append(max_rew)\n",
    "            \n",
    "    #append rew_cum, log_odd across paths to new variable\n",
    "    rew_cums = torch.cat(rew_cums)\n",
    "    if cumsum:\n",
    "        rew_cums = (rew_cums - rew_cums.mean()) / (rew_cums.std() + 1e-5) #easy baseline, not fancy one CHECK AXIS FOR MULTIPLE\n",
    "    \n",
    "    rew_cums = Variable(rew_cums)\n",
    "    \n",
    "    log_odds = [item for sublist in log_odds for item in sublist]\n",
    "    log_odds = torch.cat(log_odds).squeeze()\n",
    "    \n",
    "    if len(log_odds.shape) > 1:\n",
    "        log_odds = log_odds.sum(1)\n",
    "    #rew_cums = rew_cums.unsqueeze_(1) BAD BAD do not do\n",
    "    #log_odds = torch.cat(log_odds)\n",
    "    \n",
    "    policy_loss = -1 * (rew_cums * log_odds)\n",
    "    \n",
    "    policy_loss = torch.sum(policy_loss) / num_paths\n",
    "\n",
    "    #policy_loss = torch.sum(policy_loss,0) / num_paths\n",
    "\n",
    "    #calculate  -log prob * rewards for loss\n",
    "    #add in baseline subtractions --> this is a sepperately fitted value network, look at berkeley cs294 slides\n",
    "\n",
    "    #take optimizer step\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "if visualize:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%animate_interval==0)\n",
    "env._max_episodes_steps = min_timesteps_per_batch\n",
    "\n",
    "\n",
    "# Is this env continuous, or discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "#Get observation and action space dimensions\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "# Maximum length for episodes\n",
    "max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "#Make network object\n",
    "if not discrete:\n",
    "    output_activation = 'none'\n",
    "    act_dim *= 2\n",
    "else:\n",
    "    output_activation = 'softmax'\n",
    "net = mlp(activation = 'relu', n_layers = n_layers, input_size = obs_dim, size = size, output_size = act_dim, output_activation = output_activation)\n",
    "print(net)\n",
    "\n",
    "#Make optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "avg_reward = 0\n",
    "avg_rewards = []\n",
    "episodes = 0\n",
    "\n",
    "\n",
    "for itr in tqdm(range(n_iter)): #loop for number of optimization setps\n",
    "    paths = []\n",
    "    steps = 0\n",
    "    \n",
    "    while True: #loop to get enough timesteps in this batch --> break condition, however many games\n",
    "        ob = env.reset()\n",
    "        obs, acs, rewards, log_odds = [], [], [], []\n",
    "        animate_this_episode=(len(paths)==0 and (itr % animate_interval == 0) and animate)\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "            \n",
    "            ob_th = torch.from_numpy(ob).float().unsqueeze(0)\n",
    "            ob_th = Variable(ob_th)\n",
    "            obs.append(ob_th)\n",
    "            \n",
    "            output = net(ob_th)\n",
    "\n",
    "            ac, log_odd = sample_action(output, discrete)\n",
    "            ac = ac.data[0]\n",
    "            \n",
    "            acs.append(ac)\n",
    "            log_odds.append(log_odd)\n",
    "            \n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                episodes = episodes + 1\n",
    "                break\n",
    "                \n",
    "        path = {\"observation\" : obs, \n",
    "                \"reward\" : np.array(rewards), \n",
    "                \"action\" : (acs),\n",
    "                \"log_odds\" : log_odds}\n",
    "        \n",
    "        paths.append(path)\n",
    "        \n",
    "        if steps > min_timesteps_per_batch:\n",
    "            break #not currenlty using minimum batch size\n",
    "        \n",
    "    update_policy(paths, net, cumsum)\n",
    "    \n",
    "    if itr == 0:\n",
    "        avg_reward = path['reward'].sum()\n",
    "    else:\n",
    "        avg_reward = avg_reward * 0.9 + 0.1 * path['reward'].sum()\n",
    "    \n",
    "    if avg_reward > 900:\n",
    "        break\n",
    "        \n",
    "    avg_rewards.append(avg_reward)\n",
    "    if itr % logging_interval == 0:\n",
    "        print('Average reward: {}'.format(avg_reward))\n",
    "    #if avg_reward > env.spec.reward_threshold:\n",
    "    #    print(\"Took: {} steps to solve with reward: {}\".format(itr, avg_reward))\n",
    "    #    break\n",
    "        \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG accessory functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "#sync weights between training and target networks\n",
    "#----------------------------------------------------\n",
    "def weightSync(target_model, source_model, tau = 0.001):\n",
    "    for parameter_target, parameter_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        parameter_target.data.copy_((1 - tau) * parameter_target.data + tau * parameter_source.data)\n",
    "    #for parameter_target, parameter_source in zip(list(target_model.parameters()), list(source_model.parameters())):\n",
    "        #    parameter_target = parameter_target * (1 - tau) + parameter_source * tau\n",
    "            #does it need to be returned? I think not, everything by pointers\n",
    "\n",
    "\n",
    "#----------------------------------------------------\n",
    "#replay memory\n",
    "#----------------------------------------------------\n",
    "#from http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#replay-memory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity = 1e6):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = int(0)\n",
    "\n",
    "    def capacity(self):\n",
    "        return self.capacity\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = int((self.position + 1) % self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size <= self.__len__():\n",
    "            return random.sample(self.memory, batch_size)\n",
    "        else:\n",
    "            print('Tried to sample more samples than are in buffer')\n",
    "            return -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    \n",
    "#----------------------------------------------------\n",
    "#noise from paper\n",
    "#----------------------------------------------------\n",
    "class OrnsteinUhlenbeckProcess(object):\n",
    "    def __init__(self, dimension, num_steps, theta=0.15, mu=0, sigma=0.2, dt=0.01):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dimension = dimension\n",
    "        self.dt = dt\n",
    "        self.num_steps = num_steps\n",
    "        self.counter = 0\n",
    "        self.reset()\n",
    " \n",
    "    def step(self):\n",
    "        #scale = np.exp(-self.counter * 2.3 / self.num_steps)\n",
    "        self.x = self.x + self.theta*(self.mu-self.x)*self.dt + self.sigma*np.sqrt(self.dt)*np.random.randn(self.dimension)# * scale\n",
    "        return self.x\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = np.zeros(self.dimension)\n",
    "        #self.counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient\n",
    "###  1. Define actor and critic networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "#actor model, MLP\n",
    "#----------------------------------------------------\n",
    "#2 hidden layers, 400 units per layer, tanh output from paper to bound outputs between -1 and 1\n",
    "\n",
    "class actor(nn.Module):\n",
    "    def __init__(self,  \n",
    "            input_size, \n",
    "            output_size,\n",
    "            n_layers=2, \n",
    "            size=400, \n",
    "            activation='relu',\n",
    "            output_activation='tanh',\n",
    "            learning_rate = 1e-4):\n",
    "        \n",
    "        super(actor, self).__init__()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.architecture = []\n",
    "        for i in range(n_layers):\n",
    "            self.architecture.append(size)\n",
    "            self.architecture.append(activation)\n",
    "        self.architecture.append(output_size)\n",
    "        self.architecture.append(output_activation)\n",
    "        \n",
    "        final_index = i+1\n",
    "        #print('Network architecture is : {}'.format(self.architecture))\n",
    "        \n",
    "        #construct network\n",
    "        input_channels = input_size \n",
    "        self.layers = []\n",
    "        for idx, layer in enumerate(self.architecture):\n",
    "            output_channels = layer\n",
    "            if layer == 'tanh':\n",
    "                self.layers.append(nn.Tanh())  #NOT INPLACE? MIGHT CAUSE ISSUE OR BE OK?\n",
    "            elif layer == 'relu':\n",
    "                self.layers.append(nn.ReLU(inplace=True))\n",
    "            elif layer == 'softmax':\n",
    "                self.layers.append(nn.ReLU(inplace=True))\n",
    "            elif layer == None:\n",
    "                pass\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(input_channels, output_channels))\n",
    "                if idx<=final_index:\n",
    "                    self.layers.append(nn.BatchNorm1d(output_channels))\n",
    "                input_channels = output_channels\n",
    "        \n",
    "        self.fc1 = nn.Sequential(*self.layers)\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for k in self.modules():\n",
    "            if isinstance(k, nn.Linear):\n",
    "                m = k\n",
    "                #print('Initializing: {}'.format(m))\n",
    "                nn.init.xavier_uniform(m.weight.data) #m.weight.data.normal_(0, 0.1), paper used straight fanin, slight difference\n",
    "                m.bias.data.zero_()\n",
    "        m.weight.data.normal_(-3e-3, 3e-3) #from paper\n",
    "        m.bias.data.normal_(-3e-3, 3e-3) #from paper\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n",
    "    \n",
    "    def lr(self):\n",
    "        return self.learning_rate\n",
    "    \n",
    "#----------------------------------------------------\n",
    "#critic model, MLP\n",
    "#----------------------------------------------------\n",
    "#2 hidden layers, 300 units per layer, ouputs rewards therefore unbounded\n",
    "\n",
    "class critic(nn.Module):\n",
    "    def __init__(self,  \n",
    "            state_size,\n",
    "            action_size,\n",
    "            output_size,\n",
    "            n_layers=2, \n",
    "            size=300, \n",
    "            activation='relu',\n",
    "            output_activation= None,\n",
    "            learning_rate = 1e-3):\n",
    "        \n",
    "        super(critic, self).__init__()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.architecture = []\n",
    "        for i in range(n_layers):\n",
    "            self.architecture.append(size)\n",
    "            self.architecture.append(activation)\n",
    "        self.architecture.append(output_size)\n",
    "        self.architecture.append(output_activation)\n",
    "        \n",
    "        #print('Network architecture is : {}'.format(self.architecture))\n",
    "        \n",
    "        #construct network\n",
    "        input_channels = state_size\n",
    "        \n",
    "        self.layers = []\n",
    "        for index, layer in enumerate(self.architecture):\n",
    "            output_channels = layer\n",
    "            if layer == 'tanh':\n",
    "                self.layers.append(nn.Tanh())  #NOT INPLACE? MIGHT CAUSE ISSUE OR BE OK?      \n",
    "            elif layer == 'relu':\n",
    "                self.layers.append(nn.ReLU(inplace=True))\n",
    "            elif layer == 'softmax':\n",
    "                self.layers.append(nn.ReLU(inplace=True))\n",
    "            elif layer == None:\n",
    "                pass\n",
    "            else:\n",
    "                if index == 2:\n",
    "                    break\n",
    "                self.layers.append(nn.Linear(input_channels, output_channels))\n",
    "                self.layers.append(nn.BatchNorm1d(output_channels))\n",
    "                input_channels = output_channels\n",
    "   \n",
    "        self.fc1 = nn.Sequential(*self.layers)            \n",
    "        \n",
    "        self.layers = []\n",
    "        base_ind = index\n",
    "        for index, layer in enumerate(self.architecture[base_ind:]):\n",
    "            output_channels = layer\n",
    "            if layer == 'tanh':\n",
    "                self.layers.append(nn.Tanh())  #NOT INPLACE? MIGHT CAUSE ISSUE OR BE OK?      \n",
    "            elif layer == 'relu':\n",
    "                self.layers.append(nn.ReLU(inplace=True))\n",
    "            elif layer == 'softmax':\n",
    "                self.layers.append(nn.ReLU(inplace=True))\n",
    "            elif layer == None:\n",
    "                pass\n",
    "            else:\n",
    "                if index == 0:\n",
    "                    input_channels = output_channels + action_size\n",
    "                self.layers.append(nn.Linear(input_channels, output_channels))\n",
    "                #self.layers.append(nn.BatchNorm1d(output_channels))\n",
    "                input_channels = output_channels\n",
    "                \n",
    "        self.fc2 = nn.Sequential(*self.layers)\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for k in self.modules():\n",
    "            if isinstance(k, nn.Linear):\n",
    "                m = k\n",
    "                #print('Initializing: {}'.format(m))\n",
    "                nn.init.xavier_uniform(m.weight.data) #paper used straight fanin, not xavier\n",
    "                m.bias.data.zero_()\n",
    "        m.weight.data.uniform_(-3e-4, 3e-4) #from paper\n",
    "        m.bias.data.uniform_(-3e-4, 3e-4)  #from paper\n",
    "                \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        b = ['5', '6']\n",
    "        a,b = b\n",
    "        print(a)\n",
    "        print(b)\n",
    "        '''\n",
    "        \n",
    "        x, y = x\n",
    "        x = self.fc1(x)\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def lr(self):\n",
    "        return self.learning_rate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define DDPG object to encapsulate definition, rollouts, and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "class DDPG(object):\n",
    "    def __init__(self, obs_dim, act_dim, critic_lr = 1e-3, actor_lr = 1e-4, gamma = GAMMA, batch_size = BATCH_SIZE):\n",
    "        \n",
    "        super(DDPG, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.gamma = GAMMA\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        #actor\n",
    "        self.actor = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_target = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        #critic\n",
    "        self.critic = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic_target = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        #optimizers\n",
    "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr = 1e-4)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr = 1e-3, weight_decay=1e-2)\n",
    "        \n",
    "        #critic loss\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "        \n",
    "        #replay buffer\n",
    "        self.replayBuffer = ReplayMemory()\n",
    "        \n",
    "        #noise\n",
    "        self.noise = OrnsteinUhlenbeckProcess(dimension = act_dim, num_steps = NUM_EPISODES)\n",
    "        \n",
    "        #transition name dictionary\n",
    "        self.Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        #self.actor.train()\n",
    "        #self.critic.train()\n",
    "        #self.actor_target.train()\n",
    "        #self.critic_target.train()\n",
    "\n",
    "\n",
    "        transitions = self.replayBuffer.sample(self.batch_size)\n",
    "        batch = self.Transition(*zip(*transitions))\n",
    "\n",
    "\n",
    "        #with accessory_functions.Timer('tuple to list, loop'):\n",
    "        #    a = [element for element in batch.state]\n",
    "        #with accessory_functions.Timer('tuple to list, function'):\n",
    "        #    a = list(batch.state)\n",
    "        #--> about the same speed\n",
    "\n",
    "\n",
    "        state_batch = Variable(torch.cat(batch.state)).type(FloatTensor)\n",
    "        action_batch = Variable(torch.cat(batch.action)).type(FloatTensor)\n",
    "        reward_batch = Variable(torch.cat(batch.reward)).type(FloatTensor)\n",
    "        next_state_batch = Variable(torch.cat(batch.next_state)).type(FloatTensor)\n",
    "        done_batch = Variable(1 - torch.cat(batch.done)).type(FloatTensor)\n",
    "\n",
    "\n",
    "        #update critic\n",
    "        predicted_action_batch = self.actor_target(next_state_batch)\n",
    "        y_batch = reward_batch + self.gamma * self.critic_target(\n",
    "            [next_state_batch, predicted_action_batch]).detach() * done_batch #prevents errant backprop\n",
    "\n",
    "        #critic optimizer step\n",
    "        loss_critic = self.critic_loss(self.critic([state_batch, action_batch]), y_batch)\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        #update actor\n",
    "        loss_actor = -1 * (self.critic([state_batch, self.actor(state_batch)])).mean()\n",
    "\n",
    "        #actor optimizer step\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "        #sychronize target network with fast moving one\n",
    "        weightSync(self.critic_target, self.critic)\n",
    "        weightSync(self.actor_target, self.actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create an instance of your DDPG object\n",
    "### 3.2 Print network architectures, confirm they are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = DDPG(obs_dim = obs_dim, act_dim = act_dim)\n",
    "\n",
    "print(ddpg.actor)\n",
    "print(ddpg.critic)\n",
    "\n",
    "print(ddpg.actor.parameters)\n",
    "\n",
    "    \n",
    "for parameter in ddpg.actor.parameters():\n",
    "    print(parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying multithread from:\n",
    "    http://chriskiehl.com/article/parallelism-in-one-line/\n",
    "    https://stackoverflow.com/questions/2846653/how-to-use-threading-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpgObjects = []\n",
    "for i in range(4):\n",
    "    ddpgObjects.append(DDPG(obs_dim = obs_dim, act_dim = act_dim))\n",
    "\n",
    "\n",
    "def training_function(ddpg):\n",
    "    env = gym.make(env_name)\n",
    "    if VISUALIZE:\n",
    "        if not os.path.exists(logdir):\n",
    "            os.mkdir(logdir)\n",
    "        env = wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "    env = NormalizeAction(env)\n",
    "    env._max_episodes_steps = MAX_PATH_LENGTH\n",
    "\n",
    "\n",
    "\n",
    "    num_steps = 0\n",
    "    avg_val = 0\n",
    "    running_rewards = []\n",
    "    running_steps = []\n",
    "    for itr in range(NUM_EPISODES):\n",
    "        ob = env.reset()\n",
    "        rewards_batch = []\n",
    "\n",
    "        animate_this_episode=(itr % animate_interval == 0) and VISUALIZE\n",
    "\n",
    "        while True:\n",
    "            ddpg.noise.reset()\n",
    "\n",
    "\n",
    "            if animate_this_episode:\n",
    "                    env.render()\n",
    "                    time.sleep(0.05)\n",
    "\n",
    "            \n",
    "            #ddpg.critic.eval()\n",
    "            #ddpg.actor_target.eval()\n",
    "            #ddpg.critic_target.eval()\n",
    "\n",
    "            #choose move\n",
    "            ob_th = Variable(torch.from_numpy(ob).unsqueeze(0)).type(FloatTensor)\n",
    "            \n",
    "            ddpg.actor.eval()\n",
    "            ac = ddpg.actor(ob_th).cpu().data.numpy().squeeze() + ddpg.noise.step()\n",
    "            ddpg.actor.train()\n",
    "            \n",
    "            old_ob = ob\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "\n",
    "            #(state, action, next_state, reward, done)\n",
    "            ddpg.replayBuffer.push(torch.from_numpy(old_ob).unsqueeze(0), torch.from_numpy(ac).unsqueeze(0), torch.from_numpy(ob).unsqueeze(0), torch.Tensor([rew]).unsqueeze(0), torch.Tensor([done]).unsqueeze(0))\n",
    "\n",
    "            num_steps += 1\n",
    "            rewards_batch.append(rew)\n",
    "\n",
    "            if len(ddpg.replayBuffer) >= BATCH_SIZE:\n",
    "                ddpg.train()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        if avg_val > 900:\n",
    "            break\n",
    "\n",
    "        running_steps.append(num_steps)\n",
    "        running_rewards.append(np.sum(rewards_batch))\n",
    "        #print(running_rewards[-1])\n",
    "        avg_val = avg_val * 0.95 + 0.05*running_rewards[-1]\n",
    "        print(avg_val)\n",
    "        \n",
    "    return running_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "pool = ThreadPool(len(ddpgObjects)) \n",
    "running_rewards_looped = pool.map(training_function, ddpgObjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot rewards over multiple training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "legend = []\n",
    "plt.figure()\n",
    "for itr, trial in enumerate(running_rewards_looped):\n",
    "    out = numpy_ewma_vectorized_v2(np.array(trial),20)\n",
    "    if len(out) < 5000:\n",
    "        plt.plot(out)\n",
    "        legend.append(itr)\n",
    "\n",
    "    outs.append(out)\n",
    "    \n",
    "\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.legend(legend)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
