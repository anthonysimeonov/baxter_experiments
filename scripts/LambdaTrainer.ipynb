{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import rospy\n",
    "import baxter_interface\n",
    "import os.path as path\n",
    "import copy\n",
    "from tqdm import tqdm_notebook as tqdmn\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ros node\n",
    "rospy.init_node('lambda_trainer')\n",
    "limb = baxter_interface.Limb('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant variables\n",
    "PLAYBACK_MODE = 'bend'\n",
    "REACH_TIME = 0.05\n",
    "file_seed = path.expanduser('~/data/moveit_data/')\n",
    "if PLAYBACK_MODE == 'bend':\n",
    "    moveit_file = file_seed + 'bend_dof_'\n",
    "else:\n",
    "    file_seed = file_seed + 'full_dof_'\n",
    "limb.set_joint_position_speed(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning related parameters\n",
    "\n",
    "H_SIZE = (20, 20)\n",
    "NUM_JOINT = 3\n",
    "MAX_ITER_NUM = 1000\n",
    "OPTIM_EPOCHS = 5\n",
    "OPTIM_BATCH_SIZE = 64\n",
    "LEARNING_RATE =0.0003\n",
    "GAMMA = 0.99\n",
    "TAU = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch imports\n",
    "from torch.autograd import Variable\n",
    "from ppo_algorithm.utils import *\n",
    "\n",
    "# ppo imports\n",
    "from ppo_algorithm.models import Policy, Value\n",
    "from ppo_algorithm.core import ppo_step, estimate_advantages\n",
    "\n",
    "# class for the PPO model\n",
    "\n",
    "class PPOPolicy(object):\n",
    "    def __init__(self, state_dim = NUM_JOINT*4, action_dim = NUM_JOINT, policy_log_std = 0):\n",
    "        self.policy_net = Policy(state_dim, action_dim, log_std = policy_log_std, hidden_size = H_SIZE)\n",
    "        self.value_net = Value(state_dim)\n",
    "        self.max_iter_num = MAX_ITER_NUM\n",
    "        self.optimizer_policy = torch.optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.optimizer_value = torch.optim.Adam(value_net.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "    def update_params(self, batch, i_iter):\n",
    "        states = torch.from_numpy(np.stack(batch.state))\n",
    "        actions = torch.from_numpy(np.stack(batch.action))\n",
    "        rewards = torch.from_numpy(np.stack(batch.reward))\n",
    "        masks = torch.from_numpy(np.stack(batch.mask).astype(np.float64))\n",
    "        if use_gpu:\n",
    "            states, actions, rewards, masks = states.cuda(), actions.cuda(), rewards.cuda(), masks.cuda()\n",
    "        values = self.value_net(Variable(states, volatile=True)).data\n",
    "        fixed_log_probs = self.policy_net.get_log_prob(Variable(states, volatile=True), Variable(actions)).data\n",
    "\n",
    "        \"\"\"get advantage estimation from the trajectories\"\"\"\n",
    "        advantages, returns = estimate_advantages(rewards, masks, values, GAMMA, TAU, use_gpu)\n",
    "\n",
    "        lr_mult = max(1.0 - float(i_iter) / self.max_iter_num, 0)\n",
    "\n",
    "        \"\"\"perform mini-batch PPO update\"\"\"\n",
    "        optim_iter_num = int(math.ceil(states.shape[0] / OPTIM_BATCH_SIZE))\n",
    "        for _ in range(OPTIM_EPOCHS):\n",
    "            perm = torch.randperm(states.shape[0])\n",
    "\n",
    "            states, actions, returns, advantages, fixed_log_probs = \\\n",
    "                states[perm], actions[perm], returns[perm], advantages[perm], fixed_log_probs[perm]\n",
    "\n",
    "            for i in range(optim_iter_num):\n",
    "                ind = slice(i * OPTIM_BATCH_SIZE, min((i + 1) * OPTIM_BATCH_SIZE, states.shape[0]))\n",
    "                states_b, actions_b, advantages_b, returns_b, fixed_log_probs_b = \\\n",
    "                    states[ind], actions[ind], advantages[ind], returns[ind], fixed_log_probs[ind]\n",
    "\n",
    "                ppo_step(self.policy_net, self.value_net, self.optimizer_policy, self.optimizer_value, 1, states_b, actions_b, returns_b,\n",
    "                         advantages_b, fixed_log_probs_b, lr_mult, LEARNING_RATE, CLIP_EPSILON, L2_REG)\n",
    "    \n",
    "    def select_action(self, state, mean_flag):\n",
    "        if mean_flag:\n",
    "            return self.policy_net(state)[0].data[0].np()\n",
    "        else:\n",
    "            return self.policy_net.select_action(state)[0].np()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a loop load all the plans and replay\n",
    "joint_angles = {}\n",
    "err = []\n",
    "xaxis = []\n",
    "# observations = []\n",
    "for file_iter in tqdmn(xrange(50), desc='Files read:'):\n",
    "    plan = pickle.load(open(moveit_file + str(file_iter) + '.pkl', 'rb'))\n",
    "    ctr = 0\n",
    "    err.append(dict())\n",
    "    err[file_iter] = {'s1':[], 'e1': [], 'w1':[]}\n",
    "    obs = []\n",
    "    for ctr in tqdmn(xrange(len(plan)), desc='Waypoints achieved:'):\n",
    "        if ctr == 0:\n",
    "            joint_names = copy.deepcopy(plan[ctr])\n",
    "#             print joint_names\n",
    "        else:\n",
    "            current_velocities = limb.joint_velocities()\n",
    "            current_torques = limb.joint_efforts()\n",
    "            for (i, joint) in enumerate(joint_names):\n",
    "                joint_angles[joint] = plan[ctr][i]\n",
    "            if ctr == 1:\n",
    "                limb.move_to_joint_positions(joint_angles)\n",
    "            else:\n",
    "                start_time = time.time()\n",
    "                while ((time.time() - start_time) < REACH_TIME):\n",
    "                    limb.set_joint_positions(joint_angles)\n",
    "            measured_angles = limb.joint_angles()\n",
    "            err[file_iter]['e1'].append(measured_angles['right_e1']-joint_angles['right_e1'])\n",
    "            err[file_iter]['s1'].append(measured_angles['right_s1']-joint_angles['right_s1'])\n",
    "            err[file_iter]['w1'].append(measured_angles['right_w1']-joint_angles['right_w1'])\n",
    "            # append all dicts to a list\n",
    "#     observations.append(obs)\n",
    "#     xaxis.append(xrange(len(plan)-1))\n",
    "#     plt.plot(xaxis, err['e1']*180/np.pi, 'b')\n",
    "#     plt.plot(xaxis, err['s1']*180/np.pi, 'r')\n",
    "#     plt.plot(xaxis, err['w1']*180/np.pi, 'g')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot commanded trajectory and error\n",
    "joint_angles = {}\n",
    "for file_iter in tqdmn(xrange(2), desc='Files read:'):\n",
    "    plan = pickle.load(open(moveit_file + str(file_iter) + '.pkl', 'rb'))\n",
    "    s1 = [row[1] for row in plan[1:]]\n",
    "    e1 = [row[3] for row in plan[1:]]\n",
    "    w1 = [row[5] for row in plan[1:]]\n",
    "    plt.figure(file_iter)\n",
    "    plt.plot(np.array(s1)*180/np.pi, 'b')\n",
    "    plt.plot(np.array(e1)*180/np.pi, 'r')\n",
    "    plt.plot(np.array(w1)*180/np.pi, 'g')\n",
    "    plt.plot(np.array(err[file_iter]['e1'])*180./np.pi, 'b--')\n",
    "    plt.plot(np.array(err[file_iter]['s1'])*180./np.pi, 'r--')\n",
    "    plt.plot(np.array(err[file_iter]['w1'])*180./np.pi, 'g--')\n",
    "    plt.legend(['e1', 's1', 'w1'])\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(np.array(err[0]['e1'])*180./np.pi, 'b')\n",
    "plt.plot(np.array(err[0]['s1'])*180./np.pi, 'r')\n",
    "plt.plot(np.array(err[0]['w1'])*180./np.pi, 'g')\n",
    "plt.legend(['e1', 's1', 'w1'])\n",
    "plt.show()\n",
    "plt.figure(1)\n",
    "plt.plot(np.array(err[1]['e1'])*180./np.pi, 'b')\n",
    "plt.plot(np.array(err[1]['s1'])*180./np.pi, 'r')\n",
    "plt.plot(np.array(err[1]['w1'])*180./np.pi, 'g')\n",
    "plt.legend(['e1', 's1', 'w1'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
