{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import rospy\n",
    "import baxter_interface\n",
    "import os.path as path\n",
    "import copy\n",
    "from tqdm import tqdm_notebook as tqdmn\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "# Taken from\n",
    "# https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'mask', 'next_state',\n",
    "                                       'reward'))\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            return Transition(*zip(*self.memory))\n",
    "        else:\n",
    "            random_batch = random.sample(self.memory, batch_size)\n",
    "            return Transition(*zip(*random_batch))\n",
    "\n",
    "    def append(self, new_memory):\n",
    "        self.memory += new_memory.memory\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ros node\n",
    "rospy.init_node('lambda_trainer')\n",
    "limb = baxter_interface.Limb('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant variables\n",
    "PLAYBACK_MODE = 'bend'\n",
    "REACH_TIME = 0.05\n",
    "file_seed = path.expanduser('~/data/moveit_data/')\n",
    "if PLAYBACK_MODE == 'bend':\n",
    "    moveit_file = file_seed + 'bend_dof_'\n",
    "else:\n",
    "    file_seed = file_seed + 'full_dof_'\n",
    "limb.set_joint_position_speed(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning related parameters\n",
    "\n",
    "H_SIZE = (20, 20)\n",
    "NUM_JOINT = 3\n",
    "MAX_ITER_NUM = 1000\n",
    "OPTIM_EPOCHS = 5\n",
    "OPTIM_BATCH_SIZE = 64\n",
    "LEARNING_RATE =0.0003\n",
    "GAMMA = 0.99\n",
    "TAU = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch imports\n",
    "from torch.autograd import Variable\n",
    "from ppo_algorithm.utils import *\n",
    "\n",
    "# ppo imports\n",
    "from ppo_algorithm.models import Policy, Value\n",
    "from ppo_algorithm.core import ppo_step, estimate_advantages\n",
    "\n",
    "# class for the PPO model\n",
    "\n",
    "class PPOPolicy(object):\n",
    "    def __init__(self, state_dim = NUM_JOINT*4, action_dim = NUM_JOINT, policy_log_std = 0):\n",
    "        self.policy_net = Policy(state_dim, action_dim, log_std = policy_log_std, hidden_size = H_SIZE)\n",
    "        self.value_net = Value(state_dim)\n",
    "        self.max_iter_num = MAX_ITER_NUM\n",
    "        self.optimizer_policy = torch.optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.optimizer_value = torch.optim.Adam(value_net.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "    def update_params(self, batch, i_iter):\n",
    "        states = torch.from_numpy(np.stack(batch.state))\n",
    "        actions = torch.from_numpy(np.stack(batch.action))\n",
    "        rewards = torch.from_numpy(np.stack(batch.reward))\n",
    "        masks = torch.from_numpy(np.stack(batch.mask).astype(np.float64))\n",
    "        if use_gpu:\n",
    "            states, actions, rewards, masks = states.cuda(), actions.cuda(), rewards.cuda(), masks.cuda()\n",
    "        values = self.value_net(Variable(states, volatile=True)).data\n",
    "        fixed_log_probs = self.policy_net.get_log_prob(Variable(states, volatile=True), Variable(actions)).data\n",
    "\n",
    "        \"\"\"get advantage estimation from the trajectories\"\"\"\n",
    "        advantages, returns = estimate_advantages(rewards, masks, values, GAMMA, TAU, use_gpu)\n",
    "\n",
    "        lr_mult = max(1.0 - float(i_iter) / self.max_iter_num, 0)\n",
    "\n",
    "        \"\"\"perform mini-batch PPO update\"\"\"\n",
    "        optim_iter_num = int(math.ceil(states.shape[0] / OPTIM_BATCH_SIZE))\n",
    "        for _ in range(OPTIM_EPOCHS):\n",
    "            perm = torch.randperm(states.shape[0])\n",
    "\n",
    "            states, actions, returns, advantages, fixed_log_probs = \\\n",
    "                states[perm], actions[perm], returns[perm], advantages[perm], fixed_log_probs[perm]\n",
    "\n",
    "            for i in range(optim_iter_num):\n",
    "                ind = slice(i * OPTIM_BATCH_SIZE, min((i + 1) * OPTIM_BATCH_SIZE, states.shape[0]))\n",
    "                states_b, actions_b, advantages_b, returns_b, fixed_log_probs_b = \\\n",
    "                    states[ind], actions[ind], advantages[ind], returns[ind], fixed_log_probs[ind]\n",
    "\n",
    "                ppo_step(self.policy_net, self.value_net, self.optimizer_policy, self.optimizer_value, 1, states_b, actions_b, returns_b,\n",
    "                         advantages_b, fixed_log_probs_b, lr_mult, LEARNING_RATE, CLIP_EPSILON, L2_REG)\n",
    "    \n",
    "    def select_action(self, state, mean_flag):\n",
    "        if mean_flag:\n",
    "            return self.policy_net(state)[0].data[0].np()\n",
    "        else:\n",
    "            return self.policy_net.select_action(state)[0].np()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a loop load all the plans and replay\n",
    "joint_angles = {}\n",
    "err = []\n",
    "xaxis = []\n",
    "# observations = []\n",
    "\n",
    "def getState(limb, names):\n",
    "    measured_pos = []\n",
    "    measured_vel = []\n",
    "    measured_torque = []\n",
    "    for name in names:\n",
    "        measured_pos.append(limb.joint_angles()[name])\n",
    "        measured_vel.append(limb.joint_velocities()[name])\n",
    "        measured_torque.append(limb.joint_efforts()[name])\n",
    "    return np.array(measured_pos), np.array(measured_vel), np.array(measured_torque)\n",
    "\n",
    "def reward(goal, measured):\n",
    "    return -(np.sum(np.array(goal) - np.array(measured))**2)\n",
    "\n",
    "def within_threshold(goal, measured, threshold):\n",
    "    if np.abs(np.array(goal) - np.array(measured)).all() < threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def action_step(initial_setpoint, delta, moving_joints, all_joints):\n",
    "    joint_angles = dict()\n",
    "    moving_ind = 0\n",
    "    for i, joint in enumerate(all_joints):\n",
    "        joint_angles[joint] = initial_setpoint[i]\n",
    "        if joint in moving_joints:\n",
    "            joint_angles[joint] = initial_setpoint[moving_ind] + delta[moving_ind]\n",
    "            moving_ind += 1\n",
    "    \n",
    "    return (joint_angles)\n",
    "            \n",
    "    \n",
    "\n",
    "def generateTrajectory(numSteps = 1000):\n",
    "    positions = []\n",
    "    for i in range(numSteps):\n",
    "        positions.append(np.clip(np.cos(float(i)*(2*np.pi)/1000)*0.5+0.55, -0, 3))\n",
    "    return positions\n",
    "\n",
    "\n",
    "# every 1:00min update policy parameters\n",
    "# after a full file playback, grab a new random file\n",
    "# full outer loop is max updates\n",
    "\n",
    "\n",
    "\n",
    "class PPO_Train():\n",
    "    def __init__(self, policy_net, traj_type, num_files, angle_threshold, moving_joints, train_episodes, \n",
    "                 active_joint='right_e1', compensated_joint='right_s1'):\n",
    "        self.policy = policy_net\n",
    "        \n",
    "        #are we using random files or 1dof sine\n",
    "        self.traj_type = traj_type\n",
    "        \n",
    "        #moveit attributes\n",
    "        self.num_files = num_files\n",
    "        self.angle_threshold = angle_threshold\n",
    "        self.moving_joints = moving_joints  #also used for sine\n",
    "        self.train_episodes = train_episodes\n",
    "        \n",
    "        #1dof attributes\n",
    "        self.active_joint = active_joint\n",
    "        self.compensated_joint = compensated_joint\n",
    "        \n",
    "        #limb object\n",
    "        self.limb = baxter_interface.Limb('right')\n",
    "    \n",
    "    def getState(self, names):\n",
    "        measured_pos = []\n",
    "        measured_vel = []\n",
    "        measured_torque = []\n",
    "        for name in names:\n",
    "            measured_pos.append(self.limb.joint_angles()[name])\n",
    "            measured_vel.append(self.limb.joint_velocities()[name])\n",
    "            measured_torque.append(self.limb.joint_efforts()[name])\n",
    "        return np.array(measured_pos), np.array(measured_vel), np.array(measured_torque)\n",
    "\n",
    "    def reward(self, goal, measured):\n",
    "        return -(np.sum(np.array(goal) - np.array(measured))**2)\n",
    "\n",
    "    def reward1d(self, goal, measured):\n",
    "        return -(np.array(goal) - np.array(measured)**2)    \n",
    "    \n",
    "    def within_threshold(self, goal, measured, threshold):\n",
    "        if np.abs(np.array(goal) - np.array(measured)).all() < threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def action_step(self, initial_setpoint, delta, moving_joints, all_joints):\n",
    "        joint_angles = dict()\n",
    "        moving_ind = 0\n",
    "        for i, joint in enumerate(all_joints):\n",
    "            joint_angles[joint] = initial_setpoint[i]\n",
    "            if joint in moving_joints:\n",
    "                joint_angles[joint] = initial_setpoint[moving_ind] + delta[moving_ind]\n",
    "                moving_ind += 1\n",
    "\n",
    "        return (joint_angles)\n",
    "    \n",
    "    def initialAngles(self, angles_dict):\n",
    "        self.limb.move_to_joint_positions(angles_dict)\n",
    "\n",
    "    def generateTrajectory(self, numSteps = 1000):\n",
    "        positions = []\n",
    "        for i in range(numSteps):\n",
    "            positions.append(np.clip(np.cos(float(i)*(2*np.pi)/1000)*0.5+0.55, -0, 3))\n",
    "        return positions    \n",
    "    \n",
    "    def train(self):\n",
    "        if self.traj_type == 'moveit':\n",
    "            for episode in range(self.train_episodes):\n",
    "                if trajectory_done:\n",
    "                    file_number = np.random.randint(self.num_files)\n",
    "                    plan = pickle.load(open(moveit_file + str(file_number) + '.pkl', 'rb'))\n",
    "                    goal_list = []\n",
    "                    for i, pt in enumerate(plan):\n",
    "                        if i == 0:\n",
    "                            joint_names = copy.deepcopy(plan[i])\n",
    "                        else:\n",
    "                            goal_list.append(pt)\n",
    "                    trajectory_done = False\n",
    "                    memory = Memory()\n",
    "\n",
    "                while goal_list:\n",
    "                    goal_current_full = goal_list.pop(0)\n",
    "                    goal_current = []\n",
    "                    \n",
    "                    #chop the dimension of the goal to num_moving_joints\n",
    "                    for i, name in enumerate(self.moving_joints):\n",
    "                        ind = joint_names.index(name)\n",
    "                        goal_current.append(goal_current_full[ind])\n",
    "                        \n",
    "                    done = False\n",
    "\n",
    "                    angles, velocities, torques = self.getState(limb, self.moving_joints)\n",
    "                    s_goal = np.hstack([np.array(goal_current)])\n",
    "                    s = np.hstack([angles.copy(), velocities.copy(), torques.copy(), s_goal.copy()])\n",
    "                    s_torch = Variable(torch.from_numpy(s)).unsqueeze(0)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    while((time.time() - start_time) < REACH_TIME):\n",
    "#                         action = policy.sample_action(s)  #proper torch conversion in and out\n",
    "                        joint_angles = self.action_step(goal_current_full, action, self.moving_joints, joint_names)\n",
    "                        self.limb.set_joint_positions(joint_angles)\n",
    "\n",
    "                    angles_new, velocities_new, torques_new = self.getState(limb, self.moving_joints)\n",
    "                    s_ = np.hstack([angles_new.copy(), velocities_new.copy(), torques_new.copy(), s_goal.copy()])\n",
    "                    r = self.reward(goal_current, angles_new)\n",
    "    #                 if within_threshold(goal_current, angles, angle_threshold):\n",
    "    #                     done = 1\n",
    "    #                     memory.push(s, a, r, s_, done)\n",
    "    #                     break\n",
    "                    done = 1\n",
    "                    memory.push(s, a, done, s_, r)\n",
    "                    s = s_\n",
    "                return (memory)\n",
    "                trajectory_done = True\n",
    "                                        \n",
    "        elif self.traj_type == 'sine':\n",
    "            iter_list = generateTrajectory(num_steps=1000)\n",
    "            goal_list = [self.limb.joint_angles()[compensated_joint]]*len(iter_list)\n",
    "                                        \n",
    "            for i, goal_current in enumerate(goal_list):\n",
    "                done = False\n",
    "\n",
    "                angles, velocities, torques = self.getState(limb, self.moving_joints)\n",
    "                s_goal = np.array([iter_list[i], goal_current])\n",
    "                s = np.hstack([angles.copy(), velocities.copy(), torques.copy(), s_goal.copy()])\n",
    "                s_torch = Variable(torch.from_numpy(s)).unsqueeze(0)\n",
    "\n",
    "                start_time = time.time()\n",
    "                while((time.time() - start_time) < REACH_TIME):\n",
    "#                     action = policy.sample_action(s)\n",
    "                    joint_angles = self.limb.joint_angles()\n",
    "                    joint_angles[active_joint] = iter_list[i]\n",
    "                    joint_angles[compensated_joint] = goal_current + action\n",
    "                    self.limb.set_joint_positions(joint_angles)\n",
    "\n",
    "                angles_new, velocities_new, torques_new = self.getState(limb, self.moving_joints)\n",
    "                s_ = np.hstack([angles_new.copy(), velocities_new.copy(), torques_new.copy(), s_goal.copy()])\n",
    "                r = self.reward1d(goal_current, angles_new[self.moving_joints.index(compensated_joint)])\n",
    "#                 if within_threshold(goal_current, angles, angle_threshold):\n",
    "#                     done = 1\n",
    "#                     memory.push(s, a, r, s_, done)\n",
    "#                     break\n",
    "                done = 1\n",
    "                memory.push(s, a, done, s_, r)\n",
    "                s = s_    \n",
    "            return (memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_done = True\n",
    "num_files = 50\n",
    "angle_threshold = 0.01\n",
    "\n",
    "# trajectory_type = 'moveit'\n",
    "# moving_joints = ['right_s1', 'right_e1', 'right_w1']\n",
    "\n",
    "trajectory_type = 'sine'\n",
    "moving_joints = ['right_s1', 'right_e1']\n",
    "NUM_JOINT=2\n",
    "\n",
    "EPISODES = 1000\n",
    "EPOCHS = 10\n",
    "\n",
    "# PPO = PPOPolicy(state_dim = NUM_JOINT*4, action_dim = NUM_JOINT, policy_log_std = 0)\n",
    "PPO = PPOPolicy(state_dim = NUM_JOINT*4, action_dim = 1, policy_log_std = 0)\n",
    "\n",
    "trainer = PPO_Train(policy_net=PPO.policy_net, traj_type=trajectory_type, num_files, angle_threshold, moving_joints, train_episodes=EPISODES, \n",
    "                 active_joint='right_e1', compensated_joint='right_s1')\n",
    "\n",
    "initial_angles = {'right_s0': , 'right_s1': , 'right_e0':, 'right_e1': 'right_w0':, 'right_w1':, 'right_w2': }\n",
    "trainer.initialAngles(initial_angles)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    batch = trainer.train()\n",
    "    PPO.update_params(batch, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot commanded trajectory and error\n",
    "joint_angles = {}\n",
    "for file_iter in tqdmn(xrange(2), desc='Files read:'):\n",
    "    plan = pickle.load(open(moveit_file + str(file_iter) + '.pkl', 'rb'))\n",
    "    s1 = [row[1] for row in plan[1:]]\n",
    "    e1 = [row[3] for row in plan[1:]]\n",
    "    w1 = [row[5] for row in plan[1:]]\n",
    "    plt.figure(file_iter)\n",
    "    plt.plot(np.array(s1)*180/np.pi, 'b')\n",
    "    plt.plot(np.array(e1)*180/np.pi, 'r')\n",
    "    plt.plot(np.array(w1)*180/np.pi, 'g')\n",
    "    plt.plot(np.array(err[file_iter]['e1'])*180./np.pi, 'b--')\n",
    "    plt.plot(np.array(err[file_iter]['s1'])*180./np.pi, 'r--')\n",
    "    plt.plot(np.array(err[file_iter]['w1'])*180./np.pi, 'g--')\n",
    "    plt.legend(['e1', 's1', 'w1'])\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(np.array(err[0]['e1'])*180./np.pi, 'b')\n",
    "plt.plot(np.array(err[0]['s1'])*180./np.pi, 'r')\n",
    "plt.plot(np.array(err[0]['w1'])*180./np.pi, 'g')\n",
    "plt.legend(['e1', 's1', 'w1'])\n",
    "plt.show()\n",
    "plt.figure(1)\n",
    "plt.plot(np.array(err[1]['e1'])*180./np.pi, 'b')\n",
    "plt.plot(np.array(err[1]['s1'])*180./np.pi, 'r')\n",
    "plt.plot(np.array(err[1]['w1'])*180./np.pi, 'g')\n",
    "plt.legend(['e1', 's1', 'w1'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
