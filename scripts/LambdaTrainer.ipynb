{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import rospy\n",
    "import baxter_interface\n",
    "import os.path as path\n",
    "import copy\n",
    "from tqdm import tqdm_notebook as tqdmn\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "# Taken from\n",
    "# https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'mask', 'next_state',\n",
    "                                       'reward'))\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            return Transition(*zip(*self.memory))\n",
    "        else:\n",
    "            random_batch = random.sample(self.memory, batch_size)\n",
    "            return Transition(*zip(*random_batch))\n",
    "\n",
    "    def append(self, new_memory):\n",
    "        self.memory += new_memory.memory\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ros node\n",
    "rospy.init_node('lambda_trainer')\n",
    "limb = baxter_interface.Limb('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant variables\n",
    "PLAYBACK_MODE = 'bend'\n",
    "REACH_TIME = 0.05\n",
    "file_seed = path.expanduser('~/data/moveit_data/')\n",
    "if PLAYBACK_MODE == 'bend':\n",
    "    moveit_file = file_seed + 'bend_dof_'\n",
    "else:\n",
    "    file_seed = file_seed + 'full_dof_'\n",
    "limb.set_joint_position_speed(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning related parameters\n",
    "\n",
    "H_SIZE = (20, 20)\n",
    "NUM_JOINT = 3\n",
    "MAX_ITER_NUM = 1000\n",
    "OPTIM_EPOCHS = 5\n",
    "OPTIM_BATCH_SIZE = 64\n",
    "LEARNING_RATE =0.0003\n",
    "GAMMA = 0.99\n",
    "TAU = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch imports\n",
    "from torch.autograd import Variable\n",
    "from ppo_algorithm.utils import *\n",
    "\n",
    "# ppo imports\n",
    "from ppo_algorithm.models import Policy, Value\n",
    "from ppo_algorithm.core import ppo_step, estimate_advantages\n",
    "\n",
    "# class for the PPO model\n",
    "\n",
    "class PPOPolicy(object):\n",
    "    def __init__(self, state_dim = NUM_JOINT*4, action_dim = NUM_JOINT, policy_log_std = 0):\n",
    "        self.policy_net = Policy(state_dim, action_dim, log_std = policy_log_std, hidden_size = H_SIZE)\n",
    "        self.value_net = Value(state_dim)\n",
    "        self.max_iter_num = MAX_ITER_NUM\n",
    "        self.optimizer_policy = torch.optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.optimizer_value = torch.optim.Adam(value_net.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "    def update_params(self, batch, i_iter):\n",
    "        states = torch.from_numpy(np.stack(batch.state))\n",
    "        actions = torch.from_numpy(np.stack(batch.action))\n",
    "        rewards = torch.from_numpy(np.stack(batch.reward))\n",
    "        masks = torch.from_numpy(np.stack(batch.mask).astype(np.float64))\n",
    "        if use_gpu:\n",
    "            states, actions, rewards, masks = states.cuda(), actions.cuda(), rewards.cuda(), masks.cuda()\n",
    "        values = self.value_net(Variable(states, volatile=True)).data\n",
    "        fixed_log_probs = self.policy_net.get_log_prob(Variable(states, volatile=True), Variable(actions)).data\n",
    "\n",
    "        \"\"\"get advantage estimation from the trajectories\"\"\"\n",
    "        advantages, returns = estimate_advantages(rewards, masks, values, GAMMA, TAU, use_gpu)\n",
    "\n",
    "        lr_mult = max(1.0 - float(i_iter) / self.max_iter_num, 0)\n",
    "\n",
    "        \"\"\"perform mini-batch PPO update\"\"\"\n",
    "        optim_iter_num = int(math.ceil(states.shape[0] / OPTIM_BATCH_SIZE))\n",
    "        for _ in range(OPTIM_EPOCHS):\n",
    "            perm = torch.randperm(states.shape[0])\n",
    "\n",
    "            states, actions, returns, advantages, fixed_log_probs = \\\n",
    "                states[perm], actions[perm], returns[perm], advantages[perm], fixed_log_probs[perm]\n",
    "\n",
    "            for i in range(optim_iter_num):\n",
    "                ind = slice(i * OPTIM_BATCH_SIZE, min((i + 1) * OPTIM_BATCH_SIZE, states.shape[0]))\n",
    "                states_b, actions_b, advantages_b, returns_b, fixed_log_probs_b = \\\n",
    "                    states[ind], actions[ind], advantages[ind], returns[ind], fixed_log_probs[ind]\n",
    "\n",
    "                ppo_step(self.policy_net, self.value_net, self.optimizer_policy, self.optimizer_value, 1, states_b, actions_b, returns_b,\n",
    "                         advantages_b, fixed_log_probs_b, lr_mult, LEARNING_RATE, CLIP_EPSILON, L2_REG)\n",
    "    \n",
    "    def select_action(self, state, mean_flag):\n",
    "        if mean_flag:\n",
    "            return self.policy_net(state)[0].data[0].np()\n",
    "        else:\n",
    "            return self.policy_net.select_action(state)[0].np()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-65a23f1f4ffb>, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-65a23f1f4ffb>\"\u001b[0;36m, line \u001b[0;32m39\u001b[0m\n\u001b[0;31m    plan = pickle.load(open(moveit_file + str(file_number) + .'.pkl', 'rb'))\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# in a loop load all the plans and replay\n",
    "joint_angles = {}\n",
    "err = []\n",
    "xaxis = []\n",
    "# observations = []\n",
    "\n",
    "def getState(limb, names):\n",
    "    measured_pos = []\n",
    "    measured_vel = []\n",
    "    measured_torque = []\n",
    "    for name in names:\n",
    "        measured_pos.append(limb.joint_angles()[name])\n",
    "        measured_vel.append(limb.joint_velocities()[name])\n",
    "        measured_torque.append(limb.joint_efforts()[name])\n",
    "    return np.array(measured_pos), np.array(measured_vel), np.array(measured_torque)\n",
    "\n",
    "def reward(goal, measured):\n",
    "    return -(np.sum(np.array(goal) - np.array(measured))**2)\n",
    "\n",
    "def within_threshold(goal, measured, threshold):\n",
    "    if np.abs(np.array(goal) - np.array(measured)).all() < threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def action_step(initial_setpoint, delta, joint_names):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# every 1:00min update policy parameters\n",
    "# after a full file playback, grab a new random file\n",
    "# full outer loop is max updates\n",
    "\n",
    "trajectory_done = True\n",
    "num_files = 50\n",
    "angle_threshold = 0.01\n",
    "moving_joints = ['right_s1', 'right_e1', 'right_w1']\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    if trajectory_done:\n",
    "        file_number = np.random.randint(num_files)\n",
    "        plan = pickle.load(open(moveit_file + str(file_number) + .'.pkl', 'rb'))\n",
    "        \n",
    "        goal_list = []\n",
    "        for i, pt in enumerate(plan):\n",
    "            if i == 0:\n",
    "                joint_names = copy.deepcopy(plan[i])\n",
    "            else:\n",
    "                goal_list.append(pt)\n",
    "                \n",
    "        trajectory_done = False\n",
    "        memory = Memory()\n",
    "        \n",
    "    while goal_list:\n",
    "        goal_current = goal_list.pop(0)\n",
    "        done = False\n",
    "        \n",
    "        angles, velocities, torques = getState(limb, moving_joints)\n",
    "        s_goal = np.array(goal_current)\n",
    "        s = np.hstack([angles, velocities, torques, s_goal])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        while((time.time() - start_time) < REACH_TIME):\n",
    "#             a = policy.sample_action(s)\n",
    "#             joint_angles = f(goal_current, a)\n",
    "            limb.set_joint_positions(joint_angles)\n",
    "    \n",
    "            angles_new, velocities_new, torques_new = getState(limb, moving_joints)\n",
    "            s_ = np.hstack([angles_new, velocities_new, torques_new, s_goal])\n",
    "            \n",
    "            r = reward(goal_current, angles_)\n",
    "            \n",
    "            if within_threshold(goal_current, angles, angle_threshold):\n",
    "                done = 1\n",
    "                memory.push(s, a, r, s_, done)\n",
    "                break\n",
    "                \n",
    "            done = 0\n",
    "            memory.push(s, a, done, s_, r)\n",
    "            s = s_\n",
    "            \n",
    "    trajectory_done = True\n",
    "        \n",
    "\n",
    "for file_iter in tqdmn(xrange(50), desc='Files read:'):\n",
    "    plan = pickle.load(open(moveit_file + str(file_iter) + '.pkl', 'rb'))\n",
    "    ctr = 0\n",
    "    err.append(dict())\n",
    "    err[file_iter] = {'s1':[], 'e1': [], 'w1':[]}\n",
    "    obs = []\n",
    "    for ctr in tqdmn(xrange(len(plan)), desc='Waypoints achieved:'):\n",
    "        if ctr == 0:\n",
    "            joint_names = copy.deepcopy(plan[ctr])\n",
    "#             print joint_names\n",
    "        else:\n",
    "            current_velocities = limb.joint_velocities()\n",
    "            current_torques = limb.joint_efforts()\n",
    "            for (i, joint) in enumerate(joint_names):\n",
    "                joint_angles[joint] = plan[ctr][i]\n",
    "            if ctr == 1:\n",
    "                limb.move_to_joint_positions(joint_angles)\n",
    "            else:\n",
    "                start_time = time.time()\n",
    "                while ((time.time() - start_time) < REACH_TIME):\n",
    "                    limb.set_joint_positions(joint_angles)\n",
    "            measured_angles = limb.joint_angles()\n",
    "            err[file_iter]['e1'].append(measured_angles['right_e1']-joint_angles['right_e1'])\n",
    "            err[file_iter]['s1'].append(measured_angles['right_s1']-joint_angles['right_s1'])\n",
    "            err[file_iter]['w1'].append(measured_angles['right_w1']-joint_angles['right_w1'])\n",
    "            # append all dicts to a list\n",
    "#     observations.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot commanded trajectory and error\n",
    "joint_angles = {}\n",
    "for file_iter in tqdmn(xrange(2), desc='Files read:'):\n",
    "    plan = pickle.load(open(moveit_file + str(file_iter) + '.pkl', 'rb'))\n",
    "    s1 = [row[1] for row in plan[1:]]\n",
    "    e1 = [row[3] for row in plan[1:]]\n",
    "    w1 = [row[5] for row in plan[1:]]\n",
    "    plt.figure(file_iter)\n",
    "    plt.plot(np.array(s1)*180/np.pi, 'b')\n",
    "    plt.plot(np.array(e1)*180/np.pi, 'r')\n",
    "    plt.plot(np.array(w1)*180/np.pi, 'g')\n",
    "    plt.plot(np.array(err[file_iter]['e1'])*180./np.pi, 'b--')\n",
    "    plt.plot(np.array(err[file_iter]['s1'])*180./np.pi, 'r--')\n",
    "    plt.plot(np.array(err[file_iter]['w1'])*180./np.pi, 'g--')\n",
    "    plt.legend(['e1', 's1', 'w1'])\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(np.array(err[0]['e1'])*180./np.pi, 'b')\n",
    "plt.plot(np.array(err[0]['s1'])*180./np.pi, 'r')\n",
    "plt.plot(np.array(err[0]['w1'])*180./np.pi, 'g')\n",
    "plt.legend(['e1', 's1', 'w1'])\n",
    "plt.show()\n",
    "plt.figure(1)\n",
    "plt.plot(np.array(err[1]['e1'])*180./np.pi, 'b')\n",
    "plt.plot(np.array(err[1]['s1'])*180./np.pi, 'r')\n",
    "plt.plot(np.array(err[1]['w1'])*180./np.pi, 'g')\n",
    "plt.legend(['e1', 's1', 'w1'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
